{"chunks": ["DYPCET, Kolhapur                                                                                                                               1 \n \nABSTRACT \n              Modern organizations increasingly depend on large amounts of data, often stored in \ncommon formats like CSV, Excel, JSON, and PDF files. However, getting useful insights from \nthese raw datasets remains a challenge, especially for non-technical users without \nprogramming skills or knowledge of data analysis tools. \"Axnos AI - Talk to Your Data\" fills \nthis gap by offering an AI chatbot platform that allows users to upload data files and ask \nquestions in plain language, removing the need for coding or complex interfaces. By using \nadvanced AI models, the proposed system turns user questions into executable Pandas \noperations that filter, group, sort, and summarize data as needed. All queries, results, and file \nuploads are securely logged, letting users check their analysis history whenever they like. The \nproposed system uses a range of modern technologies, including asynchronous task handling, \nRESTful APIs and a user-friendly frontend made with Tailwind CSS and shadcn UI. By making \ndata analysis accessible to everyone, Axnos AI helps business users, researchers, and anyone \nworking with data make informed decisions quickly, promoting a data-driven culture across \nvarious fields. \n            All queries, results, and file uploads are securely logged, letting users check their \nanalysis history whenever they like. The proposed system uses a range of modern technologies, \nincluding asynchronous task handling, RESTful APIs and a user-friendly frontend made with \nTailwind CSS and shadcn UI. By making data analysis accessible to everyone, Axnos AI helps \nbusiness users, researchers, and anyone working with data make informed decisions quickly, \npromoting a data-driven culture across various fields. \n \n \n \n \n \n \n \n \n\nDYPCET, Kolhapur                                                                                                                               2 \n \nINTRODUCTION \na. Need of the Work \n                   In today\u2019s data-driven world, individuals and organizations collect and store vast \namounts of information in formats such as CSV, Excel, JSON, and PDF. However, extracting \nmeaningful insights from this data remains difficult for non-technical users who lack \nprogramming or analytical experience. Traditional tools like spreadsheets, SQL queries, or \ncoding environments often create barriers, leading to dependency on technical experts and \ndelayed decisions.  \n                   Therefore, there is a strong need for an intelligent, easy-to-use platform that enables \nusers to interact with their data naturally and efficiently. Axnos AI fulfills this need by using \nadvanced Large Language Models (LLMs) to allow users to perform data analysis simply by \nasking questions in plain language, making data exploration faster, easier, and accessible to \neveryone. \nb. Limitations", " of Existing System \n1. Technical Barriers: Most existing data analysis tools require coding knowledge or \nexpertise in software like Python, SQL, or Power BI, which limits accessibility for \ngeneral users. \n2. Limited Multi-Format Support: Current systems often handle only specific file \nformats and cannot seamlessly process data from varied sources such as CSV, Excel, \nJSON, and PDF together. \n3. Lack of Conversational Interaction: Traditional tools do not provide a natural, \nquestion-based interface for exploring data, making analysis time-consuming and \ncomplex for non-technical users. \n \n \n \n \n \n \n\nDYPCET, Kolhapur                                                                                                                               3 \n \nc. Proposed System \nThe proposed system, Axnos AI \u2013 Talk to Your Data, introduces an intelligent chatbot \nplatform that allows users to analyze and visualize data by communicating through simple text \nor voice queries. Users can upload their datasets in multiple formats (CSV, Excel, JSON, PDF), \nand the system leverages Large Language Models (LLMs) to interpret user queries and \ngenerate \nrelevant \nPython \n(Pandas-based) \noperations \nautomatically. \nThe system then executes this code securely and presents clear results in tabular or visual form, \nalong with the corresponding generated code for transparency. \nKey Features: \n\u2022 \nLLM-Based Query Interpretation: Converts user prompts into executable data \noperations without requiring programming knowledge. \n\u2022 \nMulti-Format Data Support: Efficiently handles CSV, Excel, JSON, and PDF files \nwith automatic validation and preprocessing. \n\u2022 \nSecure & Transparent Execution: Runs code in a sandboxed environment while \ndisplaying results and generated code to users. \n\u2022 \nUser History Management: Maintains logs of uploaded files, queries, and outputs for \nfuture reference. \n\u2022 \nUser-Friendly Interface: Built with React, Tailwind CSS, and shadcn UI to ensure an \nintuitive and responsive experience. \n \n \n \n \n\nDYPCET, Kolhapur                                                                                                                               4 \n \nLITERATURE SURVEY \n \nWang et al. proposed an NLI for interactive data analysis. The system recommends context-\naware next-step queries to guide novice analysts during step-bystep exploration. It also \norganizes query histories and results into dashboards, showing improved effectiveness \ncompared to a non-recommendation baseline [1]. However, it focuses more on query \nsuggestions than on multi-format ingestion or remembering analyses across sessions. \n Quamar et al. surveyed NLID approaches, including rule-based systems, text-toSQL, and \nhybrids. They outlined challenges in identifying entities, linking semantics, and generating \nstructured queries. They also highlighted progress toward conversational multi-turn interfaces \nand evaluation benchmarks. The monograph maps out the design space and open issues but \ndoes not i", "nclude heterogeneous file analysis or persistent interaction trails [2].  \nThe IEEE work linked via 9699035 addresses conversational analytics for multiturn natural \nlanguage interaction in analytical workflows. It demonstrates accessibility gains for non-\ntechnical stakeholders but often relies on predefined schemas or visualization grammars. This \ncreates gaps in generalized onboarding for various file types and in-session provenance unless \nsuch features are specifically designed [3].  \nTalk2Data allows for exploratory visual analysis through question decomposition. It breaks \ndown complex intents into sub-questions that correspond to visual operations and uses multi-\nhop reasoning. However, its focus on decomposition and visual answerability does not cover \nbroader uploads for PDF or JSON files or track longitudinal histories of user queries and \ninsights beyond specific study settings [4].  \nThe ACL N19-1423 paper presents techniques for multi-hop or context-aware question \nprocessing to support decomposition, disambiguation, and retrieval for conversational data \nanalysis. However, these methods are tested on text-focused datasets and do not offer end-to-\nend interfaces for heterogeneous uploads, Pandas-backed code generation, or secure execution \nfor non-technical users [5].  \n \n \n\nDYPCET, Kolhapur                                                                                                                               5 \n \nThe literature highlights several approaches to natural language interfaces for data. These \ninclude query recommendation, text-to-SQL, hybrid parsing, conversational context, and \nvisual exploration through question decomposition. However, there are still common gaps. \nThese include challenges in onboarding multi-format data, maintaining cross-session histories, \nand providing secure execution that is accessible to non-technical users. \nMotivation: \n                    Today, many people and businesses keep their data in formats like CSV, Excel, \nPDF, and JSON. While saving this data is simple, understanding and using it can be difficult. \nA lot of individuals do not know how to use programming or advanced software to explore \nwhat their data means. Because of this, important information often goes unused. This \nchallenge gave rise to Axnos AI.  \nMaking Data Simple for Everyone  \nMost data tools are built for people who already know how to code or work with complex \nsoftware. This makes it hard for non-technical users such as teachers, store managers, office \nworkers, and students to use their data. Many of them want helpful insights but don\u2019t have the \nrequired skills. Axnos AI makes it possible for them to simply type a question in everyday \nEnglish and get meaningful answers. No coding or special software is needed.  \nTalking Naturally with Data  \nPeople are now familiar with talking to devices using natural language. Axnos AI applies this \nidea to working with data. Instead of scrolling through spreadsheets or w", "riting formulas, users \ncan just ask something like, \u201cWhat were the most sold items last month?\u201d and get a clear \nanswer. This way of asking makes data work feel easier and more human.  \nHelping People Use Data to Decide Better  \nWhen people can easily understand their data, they make better decisions. Axnos AI supports \nthis by removing the need for technical knowledge. It helps users rely on real facts instead of \nguesses. This leads to clearer planning, fewer mistakes, and more confident choices in everyday \nwork. Axnos AI is made to be easy to understand and use. It works with many types of files \nand keeps a record of every question and answer. \n \n \n\nDYPCET, Kolhapur                                                                                                                               6 \n \nPROBLEM STATEMENT & OBJECTIVES \n \na. Problem Statement: \n             \u201cTo develop a system for Data Science Tools that enables users to perform data \nanalysis, preprocessing, and visualization using simple text or voice prompts.\u201d \n \nb. Objectives: \n1. To design a user-friendly interface that allows data upload and interaction through natural \nlanguage or voice commands.  \n2. To implement a natural language processing (NLP) engine that interprets user queries and \nmaps them to appropriate data operations.  \n3. To automate common data science tasks such as data visualization, cleaning, and summary \nstatistics generation through prompt-based interaction.  \n4. To provide real-time feedback and responses to user queries by integrating backend data \nprocessing with the front-end interface.  \n5. To support multiple data formats (e.g., CSV, Excel, JSON, PDF) and abstract away technical \ncomplexities for users with minimal data science knowledge. \n \n \n \n \n \n \n \n \n \n\nDYPCET, Kolhapur                                                                                                                               7 \n \nREQUIREMENT ANALYSIS \na. Dataset Description \n(ADD DETAILED DESCRIPTION ABOUT DATASET LIKE \nATTRIBUTES IN DATASET, FEATURES, NUMBER OF \nRECORDS IN DATASET, SOURCE LINK) \nb. Hardware Requirements \n\u2022 Processor: Intel i3 or higher/ Ryzen 3 or higher \n\u2022 Memory: Min 4GB RAM \n \nc. Software Requirements \n\u2022 Operating System: Linux/Windows.  \n\u2022 AI Tools: OpenRouter, Gemini.  \n\u2022 Programming Languages: Python, JavaScript, Typescript, HTML, CSS.  \n\u2022 Frameworks: React, Node.js, FastAPI, Nest.js, Django.  \n\u2022 Databases: Neon, Supabase, MongoDB, PostgreSQL, Redis.  \n\u2022 Tools: Docker, Git & Github, Postman, Transformer, Vercel, Render.  \n\u2022 Styling: Tailwind CSS, Shadcn UI. \nd. Libraries Used \nPython Libraries:  \n1. Pandas - A Python library that provides high-performance, easy- to- use data structures \nlike DataFrame for drawing, transubstantiating, and assaying labeled, irregular data.  \n2. NumPy - The main scientific computing package in Python, offering fast n- dimensional \narrays and vectorized operations for calculation, direct algebra, and arbitrary slice.  \n3. Se", "aborn - A statistical visualization library erected on Matplotlib. It provides pandas-\nfriendly, dataset- acquainted APIs with seductive defaults for exploratory plots.  \n4. Matplotlib - A plotting library for creating static, animated, and interactive \nvisualizations with both py-plot and object-oriented interfaces.  \n\nDYPCET, Kolhapur                                                                                                                               8 \n \n\u25cf JavaScript Libraries:  \n1. React - An element- grounded JavaScript library for erecting dynamic stoner interfaces \nwith JSX and a virtual DOM. It uses one- way data inflow and hooks- grounded state \noperation.  \n2. Redux - A state vessel that keeps app state consolidated in a single store. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\nDYPCET, Kolhapur                                                                                                                               9 \n \nSYSTEM DESIGN & IMPLEMENTATION \n   a. System Architecture \n \nFig.1 Proposed Architecture Diagram \n \n \n\nDYPCET, Kolhapur                                                                                                                               10 \n \n\u2022 \nUser Uploads Data: \nUsers select and upload their data files via a simple web interface. The frontend \nuses React with file input components for smooth user interaction. An upload API on \nthe backend receives the file for processing. \n\u2022 \nFile Validates & Saves: \nUploaded files are checked to ensure they match supported formats like CSV, \nJSON or XLSX. Backend validation is performed using Python to verify file integrity. \nFiles are stored securely on cloud storage environments. \n\u2022 \nUser Inputs Prompt: \nUsers can provide queries via natural language prompts either by entering or \nusing voice to describe their desired data tasks. The frontend chat or user interface \ncaptures these input requests. This interaction initiates the data analysis workflow. \n\u2022 \nPrompt Sent to AI: \nThe user\u2019s prompt is sent from the backend to an AI language model API. The \nAI interprets the request, determines analysis goals, and drafts a plan. Python backend \nmanages communication with external LLM APIs like OpenAI. \n\u2022 \nCode Generated \nThe AI generates executable Python code tailored for analysis or visualization. \nCode can include data processing steps, statistical calculations, or charting \ncommands. LLMs such as GPT-4 drive the automated code authoring process. \n\u2022 \nCode Execution \nThe generated code is executed within a secure, sandboxed environment. \nCommon sandbox options include Docker containers, cloud VMs, or notebook \nconverters. This ensures safe handling of arbitrary code without compromising system \nintegrity. \n\u2022 \nReturn Results & Code \nThe system compiles analysis output, charts, and the executed code into a user-\nfriendly format. Results are delivered back to the frontend for easy display and review. \nUsers see both visualizations and the underlying code for transparency. ", "\n \n\nDYPCET, Kolhapur                                                                                                                               11 \n \nb. Data Flow Diagrams \n \n \nData Flow Diagrams (Level 0 & 1) \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\nDYPCET, Kolhapur                                                                                                                               12 \n \nc. Implementation Details \n\u2022 User Authentication & Registration Module \n1. Implemented secure user registration and login functionality with password hashing \nand salting for enhanced data security. \n2. Managed user sessions using JWT-based token authentication ensuring safe and \npersistent login handling. \n3. Integrated PostgreSQL via NeonDB for storing user credentials and authentication logs \nwith proper data validation. \n \n\u2022 Chat Configuration & History Management \n1. Designed chat management modules that handle chat creation, history storage, and \nretrieval per user. \n2. Added automatic chat title generation based on message context for better organization \nof chat records. \n3. Established database relations between USER_AUTH, CHAT, and PROMPT tables to \nsupport real-time chat history and prompt tracking. \n \n\u2022 Code Generation \n1. Integrated OpenAI-based code generation APIs to process user prompts and generate \ncorresponding code snippets dynamically. \n2. Updated and optimized backend models and serializers for handling prompt requests \nand storing results efficiently. \n3. Implemented result management by linking generated code outputs to user chat sessions \nwithin the database. \n \n\u2022 Frontend Integration \n1. Developed user interface modules for registration, login, and chat dashboard using \nresponsive frontend technologies. \n2. Connected frontend components with backend APIs to enable real-time chat \ncommunication and code result display. \n3. Ensured proper error handling and form validation for all authentication and chat \ninteractions on the client side. \n\nDYPCET, Kolhapur                                                                                                                               13 \n \n \n \n \n \n \n \n \n \n \n \n \nRESULT DESCRIPTION \n \n \n \nIt should include the results of system modules (till current \nimplementation) with statistical analysis and visualization (if \nany). \n \n \n \n \n\nDYPCET, Kolhapur                                                                                                                               14 \n \n \n \n \n \n \n \n \n \n \nREFERENCES \n \n[1] \nWang, X., et al. (2022). Interactive Data Analysis with Next-step Natural Language \nQuery Recommendation. \n[2] \nQuamar, A., Efthymiou, V., Lei, C., & \u00d6zcan, F. (2020). Natural Language \nInterfaces to Data. \n[3] \nShen, L., Shen, E., Luo, Y., Yang, X., Hu, X., Zhang, X., Tai, Z., & Wang, J. \n(2021). Towards Natural Language Interfaces for Data Visualization: A Survey. \nIEEE. \n      [4] \nGuo, Y., Shi, D., Guo, M., Wu, Y., Chen, Q., & Cao, N. (2022). Talk2Data: A \nNatural Langua", "ge Interface for Exploratory Visual Analysis via Question \nDecomposition. \n      [5] \nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of\nDeep Bidirectional Transformers for Language Understanding. Proceedings of \nNAACL-HLT, 1(1), 4171\u20134186. \n\nDYPCET, Kolhapur                                                                                                                               15 \n \n \n\n"], "embeddings": [[3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0], [3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0], [3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0], [3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0], [3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0], [3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0], [425.0, 425.0, 425.0, 425.0, 425.0, 425.0, 425.0, 425.0, 425.0, 425.0, 425.0, 425.0, 425.0, 425.0, 425.0, 425.0, 425.0, 425.0, 425.0, 425.0, 425.0, 425.0, 425.0, 425.0, 425.0, 425.0, 425.0, 425.0, 425.0, 425.0, 425.0, 425.0]]}